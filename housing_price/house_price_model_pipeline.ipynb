{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"In my previous notebook I did a high-level EDA on all the features - checked missing values and explored a list of numeric and categorical attributes that could be useful in training the regression model (you can find my previous notebook here if you are interested: [Predicting House Prices - Data Processing and EDA](https://www.kaggle.com/biyuyang/predicting-house-prices-data-processing-and-eda)). In this notebook, I will explore what's the best model as well as the features that contribute most to the model performances. Below are the topics in this notebook:\n* Write up a data pipeline to execute basic data transformation\n* Batch train some baseline models and pick one for further training\n* Update the data processing pipeline with model tuning and feature selection processes"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 100)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houseTrainRaw = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\nhouseTestRaw = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houseTrain = houseTrainRaw.copy()\nhouseTest = houseTestRaw.copy()","execution_count":86,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data processing\n* Turn built years into ages until current date/time\n* Add in an indicator to show if a record has missing values for certain attributes (make it optional so that can be added/dropped in grid search)\n* Categorize numeric and categorical attributes and process them separately"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom datetime import datetime as dt","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add an attribute year age \nclass YearsToAges(BaseEstimator, TransformerMixin):\n    def __init__(self, yearCols):\n        self.cols = yearCols\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        for col in self.cols:\n            X[col + 'Age'] = dt.now().year - X[col]\n            X = X.drop(columns = col).rename(columns = {col + 'Age': col})\n        return X ","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# track missing columns before imputing if needed\nclass AddMissingIndicator(BaseEstimator, TransformerMixin):\n    def __init__(self, include_missing_cols = False):\n        self.include_missing_cols = include_missing_cols\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        if self.include_missing_cols == True:\n            cols = X.columns\n            for col in cols:\n                X[col + '_MissingInd'] = pd.isna(X[col])\n            return X\n        else:\n            return X","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select numeric VS categorical attributes\nclass NumCatSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names, include_missing_cols = False):\n        self.attribute_names = attribute_names\n        self.include_missing_cols = include_missing_cols\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        if self.include_missing_cols == True:\n            missingCols = [col + '_MissingInd' for col in self.attribute_names]\n            return pd.concat([X[self.attribute_names], X[missingCols]], axis = 1)\n        else:\n            return X[self.attribute_names]","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process numeric attributes\nclass ProcessNumAttr(BaseEstimator, TransformerMixin):\n    def __init__(self, include_missing_cols = False):\n        self.include_missing_cols = include_missing_cols\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        imputer = SimpleImputer(strategy = 'median')\n        scaler = StandardScaler()\n        if self.include_missing_cols == True:\n            missingCols = [col for col in X.columns if col.endswith('_MissingInd')]\n            cols = X.drop(columns = missingCols).columns\n            XImp = imputer.fit_transform(X[cols])\n            XScale = scaler.fit_transform(XImp)\n            return np.c_[XScale, X[missingCols]]\n        \n        else:\n            XImp = imputer.fit_transform(X)\n            XScale = scaler.fit_transform(XImp)\n            return XScale","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process categorical features\nclass ProcessCatAttr(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        for col in X.columns:\n            X[col] = X[col].astype('object')\n            X.loc[X[col].isnull(), col] = 'No Feature'\n\n        encoder = OneHotEncoder(handle_unknown = 'ignore')\n        return encoder.fit_transform(X)","execution_count":104,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# put them all together\n# categorize columns\nIdCol = ['Id']\nlabel = ['SalePrice']\nnum = [\n    'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'LotArea', 'LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n    'LowQualFinSF', 'GrLivArea', 'BsmtFullBath','BsmtHalfBath', 'FullBath', 'HalfBath', 'TotRmsAbvGrd', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n    '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'OverallQual', 'OverallCond'\n]\nyrCols = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\ncat = houseTrain.drop(columns = IdCol + label + num, axis = 1).columns\n\n# Numeric attributes pipeline\nnum_pipeline = Pipeline([\n    ('years_to_ages', YearsToAges(yrCols)),\n    ('add_missing_ind', AddMissingIndicator(False)),\n    ('select_num_attr', NumCatSelector(num, False)),\n    ('process_num_attr', ProcessNumAttr(False))\n])\n\n# categorical attributes pipeline\ncat_pipeline = Pipeline([\n    ('add_missing_ind', AddMissingIndicator(False)),\n    ('select_cat_attr', NumCatSelector(cat, False)),\n    ('process_cat_attr', ProcessCatAttr())\n])\n\nfull_pipeline = FeatureUnion(\n    transformer_list = [\n        ('num_pipeline', num_pipeline),\n        ('cat_pipeline', cat_pipeline)\n    ]\n)","execution_count":105,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houseTrainClean = full_pipeline.fit_transform(houseTrain)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training\n* Split data set into train and test\n* Batch train several models and pick the best performer OR try stacking the regressors\n* Grid search with CV on hyperparameter tuning - full data and model pipeline will be used"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score \nimport time as t","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch_fit_models(xT, yT, xV, yV, models):\n\n    # initiate a dictionary to record model results\n    resultCols = [\n        'Model', 'Train Time', \n        'Train RMSE', 'Validation RMSE',\n        'Train MAE', 'Validation MAE',\n        'Train MSLE', 'Validation MSLE',\n        'Train R2', 'Validation R2'\n    ]\n\n    result = dict([(key, []) for key in resultCols])\n    \n    # batch train models\n    for model_name, model in models.items():\n        \n        result['Model'].append(model_name)\n        \n        # train model and record time laps\n        trainStart = t.process_time()\n        fit = model.fit(xT, yT)\n        trainEnd = t.process_time()\n        \n        # back fit the model on train data\n        predTrain = fit.predict(xT)\n        \n        # fit the model on validation data\n        predValid = fit.predict(xV)\n        \n        # create data for result dict\n        result['Train Time'].append(trainEnd - trainStart)\n        result['Train RMSE'].append(np.sqrt(mean_squared_error(yT, predTrain)))\n        result['Validation RMSE'].append(np.sqrt(mean_squared_error(yV, predValid)))\n        result['Train MAE'].append(mean_absolute_error(yT, predTrain))\n        result['Validation MAE'].append(mean_absolute_error(yV, predValid))\n        result['Train MSLE'].append(mean_squared_log_error(yT, predTrain))\n        result['Validation MSLE'].append(mean_squared_log_error(yV, predValid))\n        result['Train R2'].append(r2_score(yT, predTrain))\n        result['Validation R2'].append(r2_score(yV, predValid))\n        \n    # turn result dict into a df\n    dfResult = pd.DataFrame.from_dict(result)\n    \n    return dfResult","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = houseTrain[label]\nxTrain, xValid, yTrain, yValid = train_test_split(houseTrainClean, y, test_size = 0.2, random_state = 1206)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelsToFit = {\n    'Linear Regression': LinearRegression(),\n    'Ridge': Ridge(alpha = 0.1, random_state = 777),\n    'Lasso': Lasso(alpha = 0.1, random_state = 777),\n    'Elastic Net': ElasticNet(alpha = 0.1, random_state = 777),\n    'Logistic Regression': LogisticRegression(random_state = 777),\n    'SVR (linear kernel)': SVR(kernel = 'linear'),\n    'Linear SVR': LinearSVR(random_state = 777),\n    'Random Forest': RandomForestRegressor(random_state = 777),\n    'AdaBoost': AdaBoostRegressor(random_state = 777),\n    'GBR': GradientBoostingRegressor(random_state = 777),\n    'Stacked Regressors': StackingRegressor(estimators = [('linear_reg', LinearRegression()), ('ridge', Ridge(alpha = 0.1, random_state = 777)), ('lasso', Lasso(alpha = 0.1, random_state = 777)), ('linear_svr', LinearSVR(random_state = 777)), ('linear_kernel_svm', SVR(kernel = 'linear')), ('rf', RandomForestRegressor(random_state = 777)), ('adaboost', AdaBoostRegressor(random_state = 777)), ('gbr', GradientBoostingRegressor(random_state = 777))], final_estimator = ElasticNet(alpha = 0.1, random_state = 777))\n}","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baselineModel = batch_fit_models(xTrain, yTrain, xValid, yValid, modelsToFit)\nbaselineModel.sort_values(by = 'Validation RMSE')","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"                  Model  Train Time     Train RMSE  Validation RMSE  \\\n10   Stacked Regressors   78.717372   11422.722336     22684.256930   \n3           Elastic Net    0.423113   28245.630203     24444.901647   \n9                   GBR    1.357595   13560.355612     24658.801811   \n0     Linear Regression    0.537870   19925.311008     25607.803601   \n2                 Lasso    2.019495   19925.503195     25648.321302   \n1                 Ridge    0.075468   20268.261732     25668.639738   \n7         Random Forest   11.372823   11839.034376     27304.956053   \n8              AdaBoost    0.819192   29066.972591     35524.322748   \n4   Logistic Regression   26.075781    1453.841746     50466.849538   \n5   SVR (linear kernel)    0.573986   75191.696486     77236.384356   \n6            Linear SVR    0.002934  165785.460553    169781.837480   \n\n        Train MAE  Validation MAE  Train MSLE  Validation MSLE  Train R2  \\\n10    8059.293835    16034.704945    0.004415         0.018836  0.979138   \n3    15829.380280    16372.713153    0.016046         0.019470  0.872441   \n9    10094.062083    17510.705870    0.006999         0.021499  0.970600   \n0    12537.830185    18275.815600    0.010525         0.038442  0.936523   \n2    12537.491673    18262.996777    0.010525         0.038395  0.936521   \n1    12867.882644    18168.493706    0.010796         0.033844  0.934319   \n7     6687.065154    18756.376233    0.003467         0.026863  0.977590   \n8    22828.976180    26215.700668    0.036070         0.052589  0.864915   \n4       98.886986    31016.476027    0.000057         0.060665  0.999662   \n5    49416.298172    51714.245622    0.127443         0.142946  0.096042   \n6   145760.925713   149546.444408    2.637312         2.708488 -3.394420   \n\n    Validation R2  \n10       0.920899  \n3        0.908144  \n9        0.906529  \n0        0.899196  \n2        0.898877  \n1        0.898717  \n7        0.885392  \n8        0.806008  \n4        0.608489  \n5        0.082987  \n6       -3.431126  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Train Time</th>\n      <th>Train RMSE</th>\n      <th>Validation RMSE</th>\n      <th>Train MAE</th>\n      <th>Validation MAE</th>\n      <th>Train MSLE</th>\n      <th>Validation MSLE</th>\n      <th>Train R2</th>\n      <th>Validation R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>Stacked Regressors</td>\n      <td>78.717372</td>\n      <td>11422.722336</td>\n      <td>22684.256930</td>\n      <td>8059.293835</td>\n      <td>16034.704945</td>\n      <td>0.004415</td>\n      <td>0.018836</td>\n      <td>0.979138</td>\n      <td>0.920899</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Elastic Net</td>\n      <td>0.423113</td>\n      <td>28245.630203</td>\n      <td>24444.901647</td>\n      <td>15829.380280</td>\n      <td>16372.713153</td>\n      <td>0.016046</td>\n      <td>0.019470</td>\n      <td>0.872441</td>\n      <td>0.908144</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GBR</td>\n      <td>1.357595</td>\n      <td>13560.355612</td>\n      <td>24658.801811</td>\n      <td>10094.062083</td>\n      <td>17510.705870</td>\n      <td>0.006999</td>\n      <td>0.021499</td>\n      <td>0.970600</td>\n      <td>0.906529</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Linear Regression</td>\n      <td>0.537870</td>\n      <td>19925.311008</td>\n      <td>25607.803601</td>\n      <td>12537.830185</td>\n      <td>18275.815600</td>\n      <td>0.010525</td>\n      <td>0.038442</td>\n      <td>0.936523</td>\n      <td>0.899196</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lasso</td>\n      <td>2.019495</td>\n      <td>19925.503195</td>\n      <td>25648.321302</td>\n      <td>12537.491673</td>\n      <td>18262.996777</td>\n      <td>0.010525</td>\n      <td>0.038395</td>\n      <td>0.936521</td>\n      <td>0.898877</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ridge</td>\n      <td>0.075468</td>\n      <td>20268.261732</td>\n      <td>25668.639738</td>\n      <td>12867.882644</td>\n      <td>18168.493706</td>\n      <td>0.010796</td>\n      <td>0.033844</td>\n      <td>0.934319</td>\n      <td>0.898717</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Random Forest</td>\n      <td>11.372823</td>\n      <td>11839.034376</td>\n      <td>27304.956053</td>\n      <td>6687.065154</td>\n      <td>18756.376233</td>\n      <td>0.003467</td>\n      <td>0.026863</td>\n      <td>0.977590</td>\n      <td>0.885392</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>AdaBoost</td>\n      <td>0.819192</td>\n      <td>29066.972591</td>\n      <td>35524.322748</td>\n      <td>22828.976180</td>\n      <td>26215.700668</td>\n      <td>0.036070</td>\n      <td>0.052589</td>\n      <td>0.864915</td>\n      <td>0.806008</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Logistic Regression</td>\n      <td>26.075781</td>\n      <td>1453.841746</td>\n      <td>50466.849538</td>\n      <td>98.886986</td>\n      <td>31016.476027</td>\n      <td>0.000057</td>\n      <td>0.060665</td>\n      <td>0.999662</td>\n      <td>0.608489</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>SVR (linear kernel)</td>\n      <td>0.573986</td>\n      <td>75191.696486</td>\n      <td>77236.384356</td>\n      <td>49416.298172</td>\n      <td>51714.245622</td>\n      <td>0.127443</td>\n      <td>0.142946</td>\n      <td>0.096042</td>\n      <td>0.082987</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Linear SVR</td>\n      <td>0.002934</td>\n      <td>165785.460553</td>\n      <td>169781.837480</td>\n      <td>145760.925713</td>\n      <td>149546.444408</td>\n      <td>2.637312</td>\n      <td>2.708488</td>\n      <td>-3.394420</td>\n      <td>-3.431126</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The results show that stacked regressors can perform better than individual regressors (although more regularization is probably needed - noticed overfitting from differences in train and validation RMSE). What about adding the missing indicators on each column? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numeric attributes pipeline\nnum_pipeline = Pipeline([\n    ('years_to_ages', YearsToAges(yrCols)),\n    ('add_missing_ind', AddMissingIndicator(True)),\n    ('select_num_attr', NumCatSelector(num, True)),\n    ('process_num_attr', ProcessNumAttr(True))\n])\n\n# categorical attributes pipeline\ncat_pipeline = Pipeline([\n    ('add_missing_ind', AddMissingIndicator(True)),\n    ('select_cat_attr', NumCatSelector(cat, True)),\n    ('process_cat_attr', ProcessCatAttr())\n])\n\nfull_pipeline = FeatureUnion(\n    transformer_list = [\n        ('num_pipeline', num_pipeline),\n        ('cat_pipeline', cat_pipeline)\n    ]\n)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houseTrainMisInd = full_pipeline.fit_transform(houseTrain)\nxTrain2, xValid2, yTrain2, yValid2 = train_test_split(houseTrainMisInd, y, test_size = 0.2, random_state = 1206)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baselineModelMisInd = batch_fit_models(xTrain2, yTrain2, xValid2, yValid2, modelsToFit)\nbaselineModelMisInd.sort_values(by = 'Validation RMSE')","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"                  Model  Train Time     Train RMSE  Validation RMSE  \\\n10   Stacked Regressors   89.843666   11989.019092     23051.310646   \n3           Elastic Net    0.600610   28226.539967     24448.015863   \n9                   GBR    1.620890   13560.355612     24959.528907   \n0     Linear Regression    0.808592   19924.570196     25598.154796   \n2                 Lasso    2.333504   19924.738758     25643.427479   \n1                 Ridge    0.109584   20216.579867     25672.721649   \n7         Random Forest   12.698627   11983.363764     27266.411583   \n8              AdaBoost    0.962867   29578.087578     36285.200380   \n4   Logistic Regression   31.752303    1453.841746     50750.322894   \n5   SVR (linear kernel)    0.642787   75009.573017     77046.461788   \n6            Linear SVR    0.004291  125400.485237    129209.266409   \n\n       Train MAE  Validation MAE  Train MSLE  Validation MSLE  Train R2  \\\n10   8525.750203    16369.628728    0.004995         0.019561  0.977019   \n3   15825.225027    16385.800362    0.016052         0.019464  0.872613   \n9   10094.062083    17600.028495    0.006999         0.021660  0.970600   \n0   12533.280740    18255.407216    0.010534         0.037967  0.936527   \n2   12533.132205    18246.190332    0.010533         0.037953  0.936526   \n1   12842.165625    18175.570583    0.010796         0.033027  0.934653   \n7    6715.997012    18817.167808    0.003509         0.026715  0.977040   \n8   23197.570566    27326.032011    0.038603         0.054089  0.860122   \n4      98.886986    31276.921233    0.000057         0.062222  0.999662   \n5   49225.290266    51529.998767    0.126522         0.141973  0.100415   \n6   98555.115838   102985.740596    0.641891         0.679263 -1.514242   \n\n    Validation R2  \n10       0.918319  \n3        0.908120  \n9        0.904236  \n0        0.899272  \n2        0.898916  \n1        0.898685  \n7        0.885715  \n8        0.797609  \n4        0.604078  \n5        0.087491  \n6       -1.566368  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Train Time</th>\n      <th>Train RMSE</th>\n      <th>Validation RMSE</th>\n      <th>Train MAE</th>\n      <th>Validation MAE</th>\n      <th>Train MSLE</th>\n      <th>Validation MSLE</th>\n      <th>Train R2</th>\n      <th>Validation R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>Stacked Regressors</td>\n      <td>89.843666</td>\n      <td>11989.019092</td>\n      <td>23051.310646</td>\n      <td>8525.750203</td>\n      <td>16369.628728</td>\n      <td>0.004995</td>\n      <td>0.019561</td>\n      <td>0.977019</td>\n      <td>0.918319</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Elastic Net</td>\n      <td>0.600610</td>\n      <td>28226.539967</td>\n      <td>24448.015863</td>\n      <td>15825.225027</td>\n      <td>16385.800362</td>\n      <td>0.016052</td>\n      <td>0.019464</td>\n      <td>0.872613</td>\n      <td>0.908120</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GBR</td>\n      <td>1.620890</td>\n      <td>13560.355612</td>\n      <td>24959.528907</td>\n      <td>10094.062083</td>\n      <td>17600.028495</td>\n      <td>0.006999</td>\n      <td>0.021660</td>\n      <td>0.970600</td>\n      <td>0.904236</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Linear Regression</td>\n      <td>0.808592</td>\n      <td>19924.570196</td>\n      <td>25598.154796</td>\n      <td>12533.280740</td>\n      <td>18255.407216</td>\n      <td>0.010534</td>\n      <td>0.037967</td>\n      <td>0.936527</td>\n      <td>0.899272</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lasso</td>\n      <td>2.333504</td>\n      <td>19924.738758</td>\n      <td>25643.427479</td>\n      <td>12533.132205</td>\n      <td>18246.190332</td>\n      <td>0.010533</td>\n      <td>0.037953</td>\n      <td>0.936526</td>\n      <td>0.898916</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ridge</td>\n      <td>0.109584</td>\n      <td>20216.579867</td>\n      <td>25672.721649</td>\n      <td>12842.165625</td>\n      <td>18175.570583</td>\n      <td>0.010796</td>\n      <td>0.033027</td>\n      <td>0.934653</td>\n      <td>0.898685</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Random Forest</td>\n      <td>12.698627</td>\n      <td>11983.363764</td>\n      <td>27266.411583</td>\n      <td>6715.997012</td>\n      <td>18817.167808</td>\n      <td>0.003509</td>\n      <td>0.026715</td>\n      <td>0.977040</td>\n      <td>0.885715</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>AdaBoost</td>\n      <td>0.962867</td>\n      <td>29578.087578</td>\n      <td>36285.200380</td>\n      <td>23197.570566</td>\n      <td>27326.032011</td>\n      <td>0.038603</td>\n      <td>0.054089</td>\n      <td>0.860122</td>\n      <td>0.797609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Logistic Regression</td>\n      <td>31.752303</td>\n      <td>1453.841746</td>\n      <td>50750.322894</td>\n      <td>98.886986</td>\n      <td>31276.921233</td>\n      <td>0.000057</td>\n      <td>0.062222</td>\n      <td>0.999662</td>\n      <td>0.604078</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>SVR (linear kernel)</td>\n      <td>0.642787</td>\n      <td>75009.573017</td>\n      <td>77046.461788</td>\n      <td>49225.290266</td>\n      <td>51529.998767</td>\n      <td>0.126522</td>\n      <td>0.141973</td>\n      <td>0.100415</td>\n      <td>0.087491</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Linear SVR</td>\n      <td>0.004291</td>\n      <td>125400.485237</td>\n      <td>129209.266409</td>\n      <td>98555.115838</td>\n      <td>102985.740596</td>\n      <td>0.641891</td>\n      <td>0.679263</td>\n      <td>-1.514242</td>\n      <td>-1.566368</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Hmmm... Looks like we should forget about adding missing indicators?"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Feature selection\n* Univariate feature selection\n* Recursive feature elimination\n* Based on model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import GenericUnivariateSelect, RFECV, SelectFromModel, f_regression, mutual_info_regression","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_selection_strategy(xT, yT, xV, yV, strats):\n    \n    # initiate a dictionary to record model results\n    resultCols = [\n        'Strategy', 'Train Time', \n        'Train RMSE', 'Validation RMSE',\n        'Train MAE', 'Validation MAE',\n        'Train MSLE', 'Validation MSLE',\n        'Train R2', 'Validation R2'\n    ]\n\n    result = dict([(key, []) for key in resultCols])\n    \n    # fit a stacked regression to data\n    estimators = [\n        ('linear_reg', LinearRegression()), \n        ('ridge', Ridge(alpha = 0.1, random_state = 777)), \n        ('lasso', Lasso(alpha = 0.1, random_state = 777)), \n        ('linear_svr', LinearSVR(random_state = 777)), \n        ('linear_kernel_svm', SVR(kernel = 'linear')), \n        ('rf', RandomForestRegressor(random_state = 777)), \n        ('adaboost', AdaBoostRegressor(random_state = 777)), \n        ('gbr', GradientBoostingRegressor(random_state = 777)) \n    ]\n\n    stackedRegressor = StackingRegressor(estimators = estimators, final_estimator = ElasticNet(alpha = 0.1, random_state = 777))\n    \n    # batch train models\n    for strat_name, strat in strats.items():\n        \n        result['Strategy'].append(strat_name)\n \n        # transform data, train model and record time laps\n    \n        trainStart = t.process_time()\n        selector = strat.fit(xT, yT)\n        xTU = selector.transform(xT)\n        xVU = selector.transform(xV)\n        fit = stackedRegressor.fit(xTU, yT)\n        trainEnd = t.process_time()\n        \n        # back fit the model on train data\n        predTrain = fit.predict(xTU)\n        \n        # fit the model on validation data\n        predValid = fit.predict(xVU)\n        \n        # create data for result dict\n        result['Train Time'].append(trainEnd - trainStart)\n        result['Train RMSE'].append(np.sqrt(mean_squared_error(yT, predTrain)))\n        result['Validation RMSE'].append(np.sqrt(mean_squared_error(yV, predValid)))\n        result['Train MAE'].append(mean_absolute_error(yT, predTrain))\n        result['Validation MAE'].append(mean_absolute_error(yV, predValid))\n        result['Train MSLE'].append(mean_squared_log_error(yT, predTrain))\n        result['Validation MSLE'].append(mean_squared_log_error(yV, predValid))\n        result['Train R2'].append(r2_score(yT, predTrain))\n        result['Validation R2'].append(r2_score(yV, predValid))\n        \n    # turn result dict into a df\n    dfResult = pd.DataFrame.from_dict(result)\n    \n    return dfResult","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featureSelectionStrats = {\n    'K Best': GenericUnivariateSelect(mutual_info_regression, 'k_best', 20),\n    'Percentile': GenericUnivariateSelect(mutual_info_regression, 'percentile', 10),\n    'RFECV': RFECV(ElasticNet(alpha = 0.1, random_state = 777), scoring = 'neg_root_mean_squared_error'),\n    'From Model': SelectFromModel(ElasticNet(alpha = 0.1, random_state = 777))\n}","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featureSelectionResults = feature_selection_strategy(xTrain, yTrain, xValid, yValid, featureSelectionStrats)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featureSelectionResults.sort_values(by = 'Validation RMSE')","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"     Strategy  Train Time    Train RMSE  Validation RMSE     Train MAE  \\\n3  From Model   38.159638  13488.516010     22306.664073   9765.206278   \n2       RFECV  329.252956  15718.798764     23810.134495  11522.956460   \n1  Percentile   32.180553  13955.537533     24930.832585  10400.242533   \n0      K Best   23.637024  18765.340214     29607.551617  14103.259149   \n\n   Validation MAE  Train MSLE  Validation MSLE  Train R2  Validation R2  \n3    15732.721621    0.006238         0.017694  0.970910       0.923511  \n2    16534.237215    0.009027         0.019851  0.960495       0.912852  \n1    17992.266918    0.007489         0.022507  0.968861       0.904456  \n0    20297.998547    0.012625         0.028432  0.943698       0.865248  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Strategy</th>\n      <th>Train Time</th>\n      <th>Train RMSE</th>\n      <th>Validation RMSE</th>\n      <th>Train MAE</th>\n      <th>Validation MAE</th>\n      <th>Train MSLE</th>\n      <th>Validation MSLE</th>\n      <th>Train R2</th>\n      <th>Validation R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>From Model</td>\n      <td>38.159638</td>\n      <td>13488.516010</td>\n      <td>22306.664073</td>\n      <td>9765.206278</td>\n      <td>15732.721621</td>\n      <td>0.006238</td>\n      <td>0.017694</td>\n      <td>0.970910</td>\n      <td>0.923511</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RFECV</td>\n      <td>329.252956</td>\n      <td>15718.798764</td>\n      <td>23810.134495</td>\n      <td>11522.956460</td>\n      <td>16534.237215</td>\n      <td>0.009027</td>\n      <td>0.019851</td>\n      <td>0.960495</td>\n      <td>0.912852</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Percentile</td>\n      <td>32.180553</td>\n      <td>13955.537533</td>\n      <td>24930.832585</td>\n      <td>10400.242533</td>\n      <td>17992.266918</td>\n      <td>0.007489</td>\n      <td>0.022507</td>\n      <td>0.968861</td>\n      <td>0.904456</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>K Best</td>\n      <td>23.637024</td>\n      <td>18765.340214</td>\n      <td>29607.551617</td>\n      <td>14103.259149</td>\n      <td>20297.998547</td>\n      <td>0.012625</td>\n      <td>0.028432</td>\n      <td>0.943698</td>\n      <td>0.865248</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Looks like the best strategy is to select based on a model of choice. The question is then which model should be the best. Will search for the best combination of everything."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Hyperparameter tuning\n* What is the best hyperparameter combination based on no missing indicators and model?\n* Do we want to include missing indicators?\n* Which model should be used for feature selection?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complete data/model pipeline\n# put them all together\n# categorize columns\nIdCol = ['Id']\nlabel = ['SalePrice']\nnum = [\n    'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'LotArea', 'LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n    'LowQualFinSF', 'GrLivArea', 'BsmtFullBath','BsmtHalfBath', 'FullBath', 'HalfBath', 'TotRmsAbvGrd', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n    '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'OverallQual', 'OverallCond'\n]\nyrCols = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\ncat = houseTrain.drop(columns = IdCol + label + num, axis = 1).columns\n\n# Numeric attributes pipeline\nnum_pipeline = Pipeline([\n    ('years_to_ages', YearsToAges(yrCols)),\n    ('add_missing_ind', AddMissingIndicator(False)),\n    ('select_num_attr', NumCatSelector(num, False)),\n    ('process_num_attr', ProcessNumAttr(False))\n])\n\n# categorical attributes pipeline\ncat_pipeline = Pipeline([\n    ('add_missing_ind', AddMissingIndicator(False)),\n    ('select_cat_attr', NumCatSelector(cat, False)),\n    ('process_cat_attr', ProcessCatAttr())\n])\n\ndata_transformation = FeatureUnion(\n    transformer_list = [\n        ('num_pipeline', num_pipeline),\n        ('cat_pipeline', cat_pipeline)\n    ]\n)\n\nfull_pipeline = Pipeline([\n    ('data_transformation', data_transformation),\n    ('feature_selection', SelectFromModel(ElasticNet(alpha = 0.1, random_state = 777)))\n])","execution_count":114,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacked regression model\nestimators = [\n        ('linear_reg', LinearRegression()), \n        ('ridge', Ridge(random_state = 777)), \n        ('lasso', Lasso(random_state = 777)), \n        ('linear_svr', LinearSVR(random_state = 777)), \n        ('linear_kernel_svm', SVR(kernel = 'linear')), \n        ('rf', RandomForestRegressor(random_state = 777)), \n        ('adaboost', AdaBoostRegressor(random_state = 777)), \n        ('gbr', GradientBoostingRegressor(random_state = 777)) \n    ]\n\nstackedRegressor = StackingRegressor(estimators = estimators, final_estimator = ElasticNet(random_state = 777))\n\nmodelParaGrid = {\n    'ridge__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n    'lasso__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n    'linear_svr__C': [1, 10, 100, 1000],\n    'linear_kernel_svm__C': [1, 10, 100, 1000],\n    'rf__n_estimators': [100, 500, 1000],\n    'rf__max_depth': [3, 5, 10],\n    'adaboost__n_estimators': [50, 100, 500],\n    'adaboost__learning_rate': [0.005, 0.01, 0.1, 1],\n    'gbr__n_estimators': [100, 500, 1000],\n    'gbr__learning_rate': [0.005, 0.01, 0.1, 1],\n    'gbr__min_samples_leaf': [5, 10, 100],\n    'final_estimator__alpha': [0.001, 0.01, 0.1, 1, 5]\n}\n\nrandomSearchStackedReg = RandomizedSearchCV(stackedRegressor, modelParaGrid, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 5, verbose = 3, n_jobs = -1)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houseTrainFinal = full_pipeline.fit_transform(houseTrain, y)\nrandomSearchStackedReg.fit(houseTrainFinal, y)","execution_count":37,"outputs":[{"output_type":"stream","text":"Fitting 5 folds for each of 5 candidates, totalling 25 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed: 16.1min finished\n","name":"stderr"},{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"RandomizedSearchCV(cv=5,\n                   estimator=StackingRegressor(estimators=[('linear_reg',\n                                                            LinearRegression()),\n                                                           ('ridge',\n                                                            Ridge(random_state=777)),\n                                                           ('lasso',\n                                                            Lasso(random_state=777)),\n                                                           ('linear_svr',\n                                                            LinearSVR(random_state=777)),\n                                                           ('linear_kernel_svm',\n                                                            SVR(kernel='linear')),\n                                                           ('rf',\n                                                            RandomForestRegressor(random_state=777)),\n                                                           ('adaboost',\n                                                            AdaBoostRegressor(random_state=777)),\n                                                           ('...\n                                        'gbr__learning_rate': [0.005, 0.01, 0.1,\n                                                               1],\n                                        'gbr__min_samples_leaf': [5, 10, 100],\n                                        'gbr__n_estimators': [100, 500, 1000],\n                                        'lasso__alpha': [0.0001, 0.001, 0.01,\n                                                         0.1, 1],\n                                        'linear_kernel_svm__C': [1, 10, 100,\n                                                                 1000],\n                                        'linear_svr__C': [1, 10, 100, 1000],\n                                        'rf__max_depth': [3, 5, 10],\n                                        'rf__n_estimators': [100, 500, 1000],\n                                        'ridge__alpha': [0.0001, 0.001, 0.01,\n                                                         0.1, 1]},\n                   scoring='neg_mean_squared_error', verbose=3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelCVResults = randomSearchStackedReg.cv_results_\nfor mean_score, params in zip(modelCVResults['mean_test_score'], modelCVResults['params']):\n    print(np.sqrt(-mean_score), params)","execution_count":40,"outputs":[{"output_type":"stream","text":"28758.22893471727 {'ridge__alpha': 1, 'rf__n_estimators': 1000, 'rf__max_depth': 10, 'linear_svr__C': 10, 'linear_kernel_svm__C': 1000, 'lasso__alpha': 0.001, 'gbr__n_estimators': 1000, 'gbr__min_samples_leaf': 100, 'gbr__learning_rate': 0.1, 'final_estimator__alpha': 1, 'adaboost__n_estimators': 100, 'adaboost__learning_rate': 0.1}\n27790.23359159506 {'ridge__alpha': 0.1, 'rf__n_estimators': 100, 'rf__max_depth': 5, 'linear_svr__C': 10, 'linear_kernel_svm__C': 10, 'lasso__alpha': 0.001, 'gbr__n_estimators': 1000, 'gbr__min_samples_leaf': 5, 'gbr__learning_rate': 0.1, 'final_estimator__alpha': 1, 'adaboost__n_estimators': 50, 'adaboost__learning_rate': 0.01}\n31121.710497464373 {'ridge__alpha': 0.001, 'rf__n_estimators': 1000, 'rf__max_depth': 3, 'linear_svr__C': 10, 'linear_kernel_svm__C': 100, 'lasso__alpha': 0.1, 'gbr__n_estimators': 100, 'gbr__min_samples_leaf': 100, 'gbr__learning_rate': 0.01, 'final_estimator__alpha': 5, 'adaboost__n_estimators': 50, 'adaboost__learning_rate': 0.1}\n29774.568592849293 {'ridge__alpha': 0.0001, 'rf__n_estimators': 100, 'rf__max_depth': 5, 'linear_svr__C': 1, 'linear_kernel_svm__C': 1000, 'lasso__alpha': 0.001, 'gbr__n_estimators': 500, 'gbr__min_samples_leaf': 5, 'gbr__learning_rate': 0.01, 'final_estimator__alpha': 0.01, 'adaboost__n_estimators': 500, 'adaboost__learning_rate': 1}\n30271.881030991313 {'ridge__alpha': 0.1, 'rf__n_estimators': 500, 'rf__max_depth': 3, 'linear_svr__C': 1000, 'linear_kernel_svm__C': 100, 'lasso__alpha': 0.1, 'gbr__n_estimators': 100, 'gbr__min_samples_leaf': 5, 'gbr__learning_rate': 0.01, 'final_estimator__alpha': 5, 'adaboost__n_estimators': 500, 'adaboost__learning_rate': 0.1}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomSearchStackedReg.best_estimator_","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"StackingRegressor(estimators=[('linear_reg', LinearRegression()),\n                              ('ridge', Ridge(alpha=0.1, random_state=777)),\n                              ('lasso', Lasso(alpha=0.001, random_state=777)),\n                              ('linear_svr', LinearSVR(C=10, random_state=777)),\n                              ('linear_kernel_svm', SVR(C=10, kernel='linear')),\n                              ('rf',\n                               RandomForestRegressor(max_depth=5,\n                                                     random_state=777)),\n                              ('adaboost',\n                               AdaBoostRegressor(learning_rate=0.01,\n                                                 random_state=777)),\n                              ('gbr',\n                               GradientBoostingRegressor(min_samples_leaf=5,\n                                                         n_estimators=1000,\n                                                         random_state=777))],\n                  final_estimator=ElasticNet(alpha=1, random_state=777))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finalize full pipeline with stack regressor\nbestStackedRegressor = randomSearchStackedReg.best_estimator_\n\n# Numeric attributes pipeline\nnum_pipeline = Pipeline([\n    ('years_to_ages', YearsToAges(yrCols)),\n    ('selector', NumCatSelector(num)),\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler())\n])\n\n# categorical attributes pipeline\ncat_pipeline = Pipeline([\n    ('selector', NumCatSelector(cat)),\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'No Feature')),\n    ('encoder', OneHotEncoder(handle_unknown = 'ignore'))\n])\n\ndata_transformation = FeatureUnion(\n    transformer_list = [\n        ('num_pipeline', num_pipeline),\n        ('cat_pipeline', cat_pipeline)\n    ]\n)\n\nfull_pipeline_updated = Pipeline([\n    ('data_transformation', data_transformation),\n    ('feature_selection', SelectFromModel(ElasticNet(alpha = 0.1, random_state = 777))),\n    ('stack_regression', bestStackedRegressor)\n])","execution_count":118,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission!\n* Fit the full pipeline on train data\n* Apply it on test data\n* Submit!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = full_pipeline_updated.fit(houseTrain, y)","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testID = houseTest['Id']\ntestPred = model.predict(houseTest)","execution_count":122,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.concat([testID, pd.DataFrame(testPred)], axis = 1)\nsubmission = submission.rename(columns = {0: 'SalePrice'})\nsubmission.to_csv('house_prices_submission_20200705.csv', index = False)","execution_count":123,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":124,"outputs":[{"output_type":"execute_result","execution_count":124,"data":{"text/plain":"        Id      SalePrice\n0     1461  122721.384134\n1     1462  168987.608089\n2     1463  185482.947471\n3     1464  190947.822630\n4     1465  181920.213896\n...    ...            ...\n1454  2915   78528.684406\n1455  2916   81364.977248\n1456  2917  166241.849165\n1457  2918  107276.353183\n1458  2919  229660.138747\n\n[1459 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1461</td>\n      <td>122721.384134</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1462</td>\n      <td>168987.608089</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1463</td>\n      <td>185482.947471</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1464</td>\n      <td>190947.822630</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1465</td>\n      <td>181920.213896</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1454</th>\n      <td>2915</td>\n      <td>78528.684406</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>2916</td>\n      <td>81364.977248</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>2917</td>\n      <td>166241.849165</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>2918</td>\n      <td>107276.353183</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>2919</td>\n      <td>229660.138747</td>\n    </tr>\n  </tbody>\n</table>\n<p>1459 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}